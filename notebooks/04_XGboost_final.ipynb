{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e8fd7f",
   "metadata": {},
   "source": [
    "# XGBoost \n",
    "\n",
    "(eXtreme Gradient Boosting) es una implementación optimizada de gradient boosting. Aprende secuencialmente minimizando una función de pérdida con regularización L1/L2 para prevenir overfitting. Usa árboles de decisión como base learners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fe22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error reading file: Error tokenizing data. C error: Expected 2 fields in line 6, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\kagglehub\\pandas_datasets.py:91\u001b[39m, in \u001b[36mload_pandas_dataset\u001b[39m\u001b[34m(handle, path, pandas_kwargs, sql_query)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     result = \u001b[43mread_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_build_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_extension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:820\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:914\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 2 fields in line 6, saw 3\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, roc_auc_score, confusion_matrix\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 1. Cargar datos de muestra\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m df_sample = \u001b[43mget_sample_data_kaggle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 2. Crear variables de patrones\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m     18\u001b[39m df = create_pattern_features(df_sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\PORTAFOLIO\\Detección de Actividades Sospechosas con SQL + Python\\notebooks\\src\\utils.py:16\u001b[39m, in \u001b[36mget_sample_data_kaggle\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33mSynthetic_Financial_datasets_log.csv\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# nombre real del archivo en Kaggle\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Descargar y cargar el dataset como DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m df = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mKaggleDatasetAdapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPANDAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msriharshaeedala/financial-fraud-detection-dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# dataset en Kaggle\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlatin1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# evita el error de utf-8\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlow_memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# mejora carga de CSV grande\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Dataset cargado desde Kaggle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m filas y \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columnas\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\kagglehub\\datasets.py:176\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_dataset\u001b[39m(\n\u001b[32m    164\u001b[39m     adapter: KaggleDatasetAdapter,\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# In the form of {owner_slug}/{dataset_slug} or {owner_slug}/{dataset_slug}/versions/{version_number}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    171\u001b[39m     hf_kwargs: Any = \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[32m    172\u001b[39m ) -> Any:  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[32m    173\u001b[39m     warnings.warn(\n\u001b[32m    174\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mload_dataset is deprecated and will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    175\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\kagglehub\\datasets.py:141\u001b[39m, in \u001b[36mdataset_load\u001b[39m\u001b[34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs, polars_frame_type, polars_kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter.PANDAS:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datasets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpandas_datasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_pandas_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_query\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter.POLARS:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolars_datasets\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\kagglehub\\pandas_datasets.py:97\u001b[39m, in \u001b[36mload_pandas_dataset\u001b[39m\u001b[34m(handle, path, pandas_kwargs, sql_query)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     96\u001b[39m     read_error_message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError reading file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(read_error_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mValueError\u001b[39m: Error reading file: Error tokenizing data. C error: Expected 2 fields in line 6, saw 3\n"
     ]
    }
   ],
   "source": [
    "from src.utils import get_sample_data, get_sample_data_kaggle\n",
    "from src.feature_engineerings import create_pattern_features\n",
    "from src.feature_engineerings import encode_categorical_features\n",
    "from src.feature_engineerings import scale_features\n",
    "from src.feature_engineerings import select_important_features\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# ============================\n",
    "# 1. Cargar datos de muestra\n",
    "# ============================\n",
    "df_sample = get_sample_data()\n",
    "\n",
    "# ============================\n",
    "# 2. Crear variables de patrones\n",
    "# ============================\n",
    "df = create_pattern_features(df_sample)\n",
    "\n",
    "# (Opcional) Análisis de patrones\n",
    "#pattern_analysis = df.groupby(['off_hours', 'suspicious_frequency'])['isFraud'].mean()\n",
    "\n",
    "# ============================\n",
    "# 3. Codificar variables categóricas\n",
    "# ============================\n",
    "df_encode, label_encoder = encode_categorical_features(df)\n",
    "\n",
    "# ============================\n",
    "# 4. Escalar datos\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test, scaler = scale_features(df_encode)\n",
    "\n",
    "# ============================\n",
    "# 5. Seleccionar características importantes\n",
    "# ============================\n",
    "feature_importance, selected_features, selector = select_important_features(X_train, y_train)\n",
    "print(\"Características seleccionadas:\", selected_features)\n",
    "\n",
    "# Dataset final\n",
    "X_train_final = X_train[selected_features]\n",
    "X_test_final  = X_test[selected_features]\n",
    "\n",
    "# ============================\n",
    "# 6. Calcular peso para balancear clases\n",
    "# ============================\n",
    "normal_count = len(y_train[y_train == 0])\n",
    "fraud_count = len(y_train[y_train == 1])\n",
    "pos_weight = normal_count / fraud_count\n",
    "\n",
    "# ============================\n",
    "# 7. Configurar modelo XGBoost\n",
    "# ============================\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    subsample=0.8,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=2.0,\n",
    "    gamma=0,\n",
    "    colsample_bytree=1.0,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=pos_weight,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 8. Entrenar modelo\n",
    "# ============================\n",
    "print(\"Entrenando XGBoost...\")\n",
    "xgb_model.fit(X_train_final, y_train)\n",
    "\n",
    "# ============================\n",
    "# 9. Evaluar modelo\n",
    "# ============================\n",
    "y_pred = xgb_model.predict(X_test_final)\n",
    "y_prob = xgb_model.predict_proba(X_test_final)[:, 1]  \n",
    "\n",
    "# Threshold personalizado\n",
    "threshold = 0.3\n",
    "y_pred_custom = (y_prob > threshold).astype(int)\n",
    "\n",
    "# Métricas\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "print(f\"Resultados XGBoost con threshold {threshold}:\")\n",
    "print(\"AUC Score:\", auc_score)\n",
    "print(confusion_matrix(y_test, y_pred_custom))\n",
    "print(classification_report(y_test, y_pred_custom))\n",
    "\n",
    "print(\"Modelo XGBoost entrenado y evaluado.\")\n",
    "print(\"Características seleccionadas:\", list(xgb_model.feature_names_in_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaa8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import joblib\n",
    "import json\n",
    "\n",
    "# === 1. Guardar modelo entrenado ===\n",
    "joblib.dump(xgb_model, 'modelo_xgboost_fraude.pkl')\n",
    "\n",
    "# === 2. Guardar scaler usado para normalización ===\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# === 3. Guardar selector de features importantes ===\n",
    "joblib.dump(selector, 'selector_kbest.pkl')\n",
    "\n",
    "# === 4. Guardar nombres de las features seleccionadas ===\n",
    "joblib.dump(selected_features, 'features_seleccionadas.pkl')\n",
    "\n",
    "# 🔹 Mejor guardar solo los nombres de columnas, no todo X_train\n",
    "columnas_entrenamiento = list(X_train.columns)\n",
    "joblib.dump(columnas_entrenamiento, 'columnas_entrenamiento.pkl')\n",
    "\n",
    "# === 5. Guardar threshold personalizado en un archivo JSON (más flexible) ===\n",
    "config = {\"threshold\": 0.3}\n",
    "with open('config_modelo.json', 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "print(\"✅ Todo fue guardado exitosamente para producción.\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3783e1",
   "metadata": {},
   "source": [
    "## DESPLIEGUE ANTIFRAUDE MODELO XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e13869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# === 1. Cargar componentes entrenados ===\n",
    "model = joblib.load('modelo_xgboost_fraude.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "selector = joblib.load('selector_kbest.pkl')\n",
    "columnas_entrenamiento = joblib.load('columnas_entrenamiento.pkl')  # lista de nombres de columnas\n",
    "\n",
    "# === 2. Cargar configuración del modelo (threshold, etc.) ===\n",
    "with open('config_modelo.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "threshold = config[\"threshold\"]\n",
    "\n",
    "# === 3. Diccionario de datos de prueba (ejemplo de fraude) ===\n",
    "data_prueba_dict = {\n",
    "    'step': 25,                \n",
    "    'amount': 120.0,           \n",
    "    'oldbalanceOrg': 1500.0,   \n",
    "    'newbalanceOrig': 1380.0,  \n",
    "    'oldbalanceDest': 500.0,   \n",
    "    'newbalanceDest': 620.0,   \n",
    "    'hour_of_day': 11,         \n",
    "    'off_hours': 0,            \n",
    "    'day_of_week': 3,          \n",
    "    'weekend_activity': 0,     \n",
    "    'amount_frequency': 1,     \n",
    "    'suspicious_frequency': 0, \n",
    "    'structured_amount': 0,    \n",
    "    'dest_diversity': 1,       \n",
    "    'high_dest_diversity': 0,  \n",
    "    'type_encoded': 3,         \n",
    "    'type_CASH_IN': 0,\n",
    "    'type_CASH_OUT': 0,\n",
    "    'type_DEBIT': 0,\n",
    "    'type_PAYMENT': 1,\n",
    "    'type_TRANSFER': 0,        \n",
    "    'type_fraud_rate': 0.01,   \n",
    "    'hour_sin': 0.91,          \n",
    "    'hour_cos': -0.41\n",
    "}\n",
    "\n",
    "# === 4. Convertir a DataFrame y alinear columnas ===\n",
    "data_prueba = pd.DataFrame([data_prueba_dict])\n",
    "\n",
    "# Alinear columnas exactamente como en el entrenamiento\n",
    "data_prueba = data_prueba[columnas_entrenamiento]\n",
    "\n",
    "# === 5. Escalar datos ===\n",
    "data_scaled = scaler.transform(data_prueba)\n",
    "\n",
    "# === 6. Seleccionar features importantes ===\n",
    "data_selected = selector.transform(data_scaled)\n",
    "\n",
    "# === 7. Obtener predicciones ===\n",
    "probabilidad = model.predict_proba(data_selected)[0][1]\n",
    "prediccion = int(probabilidad > threshold)  # umbral personalizado\n",
    "\n",
    "# === 8. Mostrar resultados ===\n",
    "print(\"¿Es fraude?:\", \"✅ SÍ\" if prediccion == 1 else \"❌ NO\")\n",
    "print(f\"Probabilidad de fraude: {probabilidad:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
